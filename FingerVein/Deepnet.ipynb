{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IX3J15lvtmuM"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "!pip install -q tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rEjMqkv6ZcBL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random \n",
    "import itertools\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.io import imread#,imshow\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.transform import resize\n",
    "from skimage.color import grey2rgb\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePum71z4csnV"
   },
   "outputs": [],
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDI-jw8bW_8L"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "Dir = os.listdir(\"./\")\n",
    "\n",
    "if (\"MC_Finger_Vein_Database\" not in Dir):\n",
    "    if (\"Finger Vein Database.rar\" not in Dir):\n",
    "        checkpoint = drive.CreateFile({'id': '1Qq2TbwoolE9BGOD8Ooqon8KK1c_AkDhs'})\n",
    "        checkpoint.GetContentFile('Finger_Vein_Database.zip')\n",
    "#         !wget http://mla.sdu.edu.cn/sjk/Finger%20Vein%20Database.rar\n",
    "        del checkpoint\n",
    "    !unzip \"Finger_Vein_Database.zip\"\n",
    "    !mv \"Finger\" \"MC_Finger_Vein_Database\"\n",
    "del Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgqLLsb2H8Xy"
   },
   "outputs": [],
   "source": [
    "DATADIR = \"MC_Finger_Vein_Database\"\n",
    "batch_size =  6*3*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDlWdkDk61B9"
   },
   "outputs": [],
   "source": [
    "def load_image(addr):    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        image = grey2rgb(imread(addr))\n",
    "#         image = image[:,5:-40]\n",
    "#         thresh = threshold_otsu(image)\n",
    "#         binary = image <= thresh\n",
    "#         image = image * convex_hull_image(binary == 0)\n",
    "#         image = equalize_adapthist(image)\n",
    "        \n",
    "#         del thresh, binary\n",
    "#         return resize(image, (224,224,3), mode='constant' )/255.0\n",
    "        return image/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36428,
     "status": "ok",
     "timestamp": 1557829281316,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "_s3ZuWSb6-4a",
    "outputId": "071f0e1f-40a2-40d0-ab30-1787be3c9914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Getting Data at-- 042 ---Remaining-- 1 "
     ]
    }
   ],
   "source": [
    "Dir = os.listdir(\"./\")\n",
    "# print(Dir)\n",
    "if (\"image_data.pickle\" not in Dir and \"test_image_data.pickle\" not in Dir):\n",
    "#     image_path = {\"left_index\":[], \"left_ring\":[], \"left_middle\":[],\n",
    "#                  \"right_index\":[], \"right_ring\":[], \"right_middle\":[]}\n",
    "#     test_image_path =  {\"left_index\":[], \"left_ring\":[], \"left_middle\":[],\n",
    "#                         \"right_index\":[], \"right_ring\":[], \"right_middle\":[]}\n",
    "    image_path = np.array([])\n",
    "    test_image_path=np.array([])\n",
    "    fingers={\"MC_index\":'1',\"MC_middle\":'2',\"MC_ring\":'3'}\n",
    "    hands = {\"left\":'1',\"right\":'2'}\n",
    "    n=len(os.listdir(DATADIR))\n",
    "    remain=0\n",
    "    for person in os.listdir(DATADIR):\n",
    "        sys.stdout.write('\\r Getting Data at-- {0} ---Remaining-- {1} '.format(str(person) ,str(n-remain)))\n",
    "        sys.stdout.flush()\n",
    "        remain+=1\n",
    "        path=os.path.join(DATADIR,person)\n",
    "        for hand in os.listdir(path):\n",
    "            path_=os.path.join(path,hand)\n",
    "            listofpath = os.listdir(path_)\n",
    "            for finger in sorted(listofpath):\n",
    "                label=int(person + hands[hand] + fingers[finger[:-6]])#001leftindex == 123\n",
    "                if(int(n-remain)<21):\n",
    "                    test_image_path = np.append(test_image_path, np.array([np.array([load_image(os.path.join(path_,finger)),label])]))\n",
    "                else:\n",
    "                    image_path = np.append(image_path, np.array([np.array([load_image(os.path.join(path_,finger)),label])]))\n",
    "\n",
    "    del person, path, hand, finger, label\n",
    "    image_path = image_path.reshape((3060, 2))\n",
    "    test_image_path = test_image_path.reshape((756,2))\n",
    "    import pickle\n",
    "    \n",
    "#     pickle_out = open(\"image_data.pickle\",\"wb\")\n",
    "#     pickle.dump(image_path, pickle_out)\n",
    "#     pickle_out.close()\n",
    "\n",
    "\n",
    "#     pickle_out = open(\"test_image_data.pickle\",\"wb\")\n",
    "#     pickle.dump(test_image_path, pickle_out)\n",
    "#     pickle_out.close()\n",
    "    \n",
    "#     uploaded = drive.CreateFile({'title' : 'mc_image_data.pickle', \"parents\": \n",
    "#                                        [{\"kind\": \"drive#childList\",\"id\": \"11el4YYRbi7xpoNRbvbgE0LDo47ygBazP\"}]})\n",
    "#     uploaded.SetContentFile('image_data.pickle')\n",
    "#     uploaded.Upload()\n",
    "    \n",
    "#     uploaded = drive.CreateFile({'title' : 'mc_test_image_path.pickle', \"parents\": \n",
    "#                                        [{\"kind\": \"drive#childList\",\"id\": \"11el4YYRbi7xpoNRbvbgE0LDo47ygBazP\"}]})\n",
    "#     uploaded.SetContentFile('test_image_path.pickle')\n",
    "#     uploaded.Upload()\n",
    "    \n",
    "else:\n",
    "    pickle_in = open(\"image_data.pickle\",\"rb\")\n",
    "    image_path = pickle.load(pickle_in)\n",
    "    \n",
    "    pickle_in = open(\"test_image_data.pickle\",\"rb\")\n",
    "    test_image_path = pickle.load(pickle_in)\n",
    "\n",
    "del Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "go7flHCE7DK4"
   },
   "outputs": [],
   "source": [
    "image_path = image_path.reshape((510,6,2))\n",
    "np.random.shuffle(image_path)\n",
    "image_path = image_path.reshape((3060,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xgVQJV9g8Ck_"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, batch_size=64, dim=(32,32,32), #n_channels=1,\n",
    "                 n_classes=10, shuffle=False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))#, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Image\n",
    "            X[i,] = image_path[ID][0]\n",
    "\n",
    "            # Label\n",
    "            y[i] =  image_path[ID][1]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36386,
     "status": "ok",
     "timestamp": 1557829281362,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "-egGXy6qoTOF",
    "outputId": "4ddd57c8-c037-43e8-802a-1329a1b45ac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 320, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfECva1g5UrS"
   },
   "outputs": [],
   "source": [
    "dataset = DataGenerator(list(range(0, len(image_path))), batch_size=batch_size, dim = image_path[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24881,
     "status": "ok",
     "timestamp": 1557829281373,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "f97pRabHyxIM",
    "outputId": "a2ea7758-8602-4e67-9093-242bbd764164"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataGenerator at 0x7f9316ef6668>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itNWFUotawhJ"
   },
   "outputs": [],
   "source": [
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def _get_anchor_positive_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i and j are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "\n",
    "    # Check if labels[i] == labels[j]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_anchor_negative_triplet_mask(labels):\n",
    "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    Returns:\n",
    "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
    "    \"\"\"\n",
    "    # Check if labels[i] != labels[k]\n",
    "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
    "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "\n",
    "    mask = tf.logical_not(labels_equal)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # shape (batch_size, batch_size, 1)\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
    "    # shape (batch_size, 1, batch_size)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss, fraction_positive_triplets\n",
    "\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Qbm45ffH8X8"
   },
   "outputs": [],
   "source": [
    "def create_embedding_network(input_shape):\n",
    "    input_ = tf.keras.layers.Input(input_shape)\n",
    "    x= tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conv1\", activation=\"relu\" , input_shape= input_shape[1:])(input_)\n",
    "    x= tf.keras.layers.MaxPooling2D(pool_size= (3, 3), strides = (1, 1), padding = \"valid\", name=\"pool1\")(x)\n",
    "\n",
    "#     x= tf.keras.layers.Conv2D(64, (1, 1), strides=(1, 1), padding = \"valid\", name=\"conv2a\", activation=\"relu\")(x)\n",
    "#     x= tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conv2\", activation=\"relu\")(x)\n",
    "#     x= tf.keras.layers.MaxPooling2D(pool_size= (2, 2), strides = (2, 2), padding = \"valid\", name=\"pool2\")(x)\n",
    "    \n",
    "    x= tf.keras.layers.Conv2D(64, (1, 1), strides=(1, 1), padding = \"valid\", name=\"conv2a\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conv2\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.MaxPooling2D(pool_size= (3, 3), strides = (2, 2), padding = \"valid\", name=\"pool2\")(x)\n",
    "\n",
    "    x= tf.keras.layers.Conv2D(128, (1, 1), strides=(1, 1), padding = \"valid\", name=\"conv3a\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conv3\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.MaxPooling2D(pool_size= (3, 3), strides = (2, 2), padding = \"valid\", name=\"pool3\")(x)\n",
    "    x= tf.keras.layers.Conv2D(256, (1, 1), strides=(1, 1), padding = \"valid\", name=\"conv4a\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conv4\", activation=\"relu\")(x)\n",
    "    \n",
    "    x= tf.keras.layers.MaxPooling2D(pool_size= (3, 3), strides = (2, 2), padding = \"valid\", name=\"pool4\")(x)\n",
    "    x= tf.keras.layers.Conv2D(256, (1, 1), strides=(1, 1), padding = \"valid\", name=\"conva5a\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), padding = \"valid\", name=\"conva5\", activation=\"relu\")(x)\n",
    "    x= tf.keras.layers.MaxPooling2D(pool_size= (3, 3), strides = (2, 2), padding = \"valid\", name=\"pool5\")(x)\n",
    "    x= tf.keras.layers.Flatten()(x)\n",
    "    x= tf.keras.layers.Dense(1024,activation=tf.nn.softmax)(x)\n",
    "    model = tf.keras.models.Model(inputs=input_, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pvqtdFfuH8YB"
   },
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    global learning_rate\n",
    "    anchor = tf.keras.layers.Input(shape=input_shape)\n",
    "    embedding_network = create_embedding_network(input_shape)\n",
    "    loss = batch_hard_triplet_loss(positive_embedding)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EI3vLsGXH8YH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2buA9JPH8YM"
   },
   "outputs": [],
   "source": [
    "# def get_triplets(features):\n",
    "#     df = pd.DataFrame(features)\n",
    "#     triplets = []\n",
    "#     for index, row in df.iterrows():\n",
    "#     same_tag = df.loc[df.iloc[:, -1] == row.iloc[-1]]\n",
    "#     same_tag_indexes = list(set(same_tag.index) - {index})\n",
    "#     diff_tag_indexes = list(set(df.index) - set(same_tag_indexes) - {index})\n",
    "#     anchor = row[0]\n",
    "#     pos = df.iloc[random.choice(same_tag_indexes), :].iloc[0]\n",
    "#     neg = df.iloc[random.choice(diff_tag_indexes), :].iloc[0]\n",
    "#     triplets.append(list(list([anchor, pos, neg])))\n",
    "#     return np.array(triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1013
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1742,
     "status": "error",
     "timestamp": 1555174503863,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "YhDOtxYx3XEP",
    "outputId": "67874e80-af4b-4cc6-b2f4-a8772e0f9a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 240, 320, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 238, 318, 64)      640       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 236, 316, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2a (Conv2D)              (None, 236, 316, 64)      4160      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 234, 314, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 116, 156, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv2D)              (None, 116, 156, 128)     8320      \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 114, 154, 128)     147584    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 56, 76, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv2D)              (None, 56, 76, 256)       33024     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 54, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 26, 36, 256)       0         \n",
      "_________________________________________________________________\n",
      "conva5a (Conv2D)             (None, 26, 36, 256)       65792     \n",
      "_________________________________________________________________\n",
      "conva5 (Conv2D)              (None, 24, 34, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool5 (MaxPooling2D)         (None, 11, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 45056)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              46138368  \n",
      "=================================================================\n",
      "Total params: 47,614,976\n",
      "Trainable params: 47,614,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-1044e8f2f181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-34ef63d6253c>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpositive_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#   model = tf.keras.models.Model(inputs=[anchor, positive, negative], outputs=loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_hard_triplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_hard_triplet_loss() missing 2 required positional arguments: 'embeddings' and 'margin'"
     ]
    }
   ],
   "source": [
    "model = build_model((240,320,1))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 177666,
     "status": "ok",
     "timestamp": 1554125858271,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "i4m4aD0CeKNk",
    "outputId": "9a36d513-e236-40cd-871b-1d4508036e76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3180/3180 [==============================] - 100s 31ms/sample - loss: 0.1434\n",
      "8/8 [==============================] - 0s 31ms/sample - loss: 0.2340\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(training_data)\n",
    "# epochs = 1\n",
    "\n",
    "# for i in range(num_epochs):\n",
    "#     hard=[]\n",
    "#     history = model.fit([data[:,0],data[:,1],data[:,2]], y=np.zeros(data[:,0].shape[0]), epochs=epochs, batch_size=1)\n",
    "#     #   epochs+=1\n",
    "#     epoch_loss = history.history['loss'][-1]\n",
    "#     file_name = 'Epoch_'+str(i)+'.h5'\n",
    "#     model.save_weights(file_name)\n",
    "#     model_file = drive.CreateFile({'title' : file_name})\n",
    "#     model_file.SetContentFile(file_name)\n",
    "#     model_file.Upload()\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "#         same_tag = df.loc[df.iloc[:, -1] == row.iloc[-1]]\n",
    "#         same_tag_indexes = list(set(same_tag.index) - {index})\n",
    "#         diff_tag_indexes = list(set(df.index) - set(same_tag_indexes) - {index})\n",
    "#         anchor = row[0]\n",
    "#         pos = df.iloc[random.choice(same_tag_indexes), :].iloc[0]\n",
    "#         choice = random.choice(diff_tag_indexes)\n",
    "#         neg = df.iloc[choice, :].iloc[0]\n",
    "#         anchor_ = anchor.reshape((1,)+anchor.shape)\n",
    "#         pos_ = pos.reshape((1,)+pos.shape)\n",
    "#         neg_ = neg.reshape((1,)+neg.shape)\n",
    "#         ans = model.predict([anchor_,pos_,neg_])\n",
    "#     if ans[0] > epoch_loss:\n",
    "#         hard.append(list(list([anchor, pos, neg])))\n",
    "#     if(len(hard)<1):\n",
    "#         break\n",
    "#     data = np.array(hard)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1554125993057,
     "user": {
      "displayName": "Nihaal Modi",
      "photoUrl": "https://lh6.googleusercontent.com/-Ua0BkKr11xs/AAAAAAAAAAI/AAAAAAAAFl4/F4qWPXkXzPg/s64/photo.jpg",
      "userId": "10109107905944308955"
     },
     "user_tz": -330
    },
    "id": "uk9kqxRsH8Y-",
    "outputId": "2f8f2312-2f6d-4e4e-a89b-716a582d269b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# st = time.time()\n",
    "# model.predict([data[:5,0],data[1900:1905,1],data[90:95,2]])\n",
    "# print(time.time()-st)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STtgbvOXH8ZC"
   },
   "outputs": [],
   "source": [
    "# ans = model.predict([data[:7,0],data[0:7,1],data[:7,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JFw0XLpo49n2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pn2Kikn74-qP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deepnet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
